{"cells":[{"cell_type":"code","execution_count":null,"id":"ab42c33c","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d3d3c2e2a84ff92fa02220ae689dc414","grade":false,"grade_id":"cell-38b4529793385ad1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ab42c33c"},"outputs":[],"source":["from random import randrange\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, log_loss\n","from sklearn.datasets import load_breast_cancer, load_diabetes\n","\n","\n","def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5, error=1e-9):\n","    \"\"\"\n","    sample a few random elements and only return numerical\n","    in this dimensions\n","    \"\"\"\n","\n","    for i in range(num_checks):\n","        ix = tuple([randrange(m) for m in x.shape])\n","\n","        oldval = x[ix]\n","        x[ix] = oldval + h  # increment by h\n","        fxph = f(x)  # evaluate f(x + h)\n","        x[ix] = oldval - h  # increment by h\n","        fxmh = f(x)  # evaluate f(x - h)\n","        x[ix] = oldval  # reset\n","\n","        grad_numerical = (fxph - fxmh) / (2 * h)\n","        grad_analytic = analytic_grad[ix]\n","        rel_error = abs(grad_numerical - grad_analytic) / (\n","            abs(grad_numerical) + abs(grad_analytic)\n","        )\n","        print(\n","            \"numerical: %f analytic: %f, relative error: %e\"\n","            % (grad_numerical, grad_analytic, rel_error)\n","        )\n","        assert rel_error < error\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","id":"cfec4298","metadata":{"id":"cfec4298"},"source":["# Linear regression"]},{"cell_type":"code","execution_count":null,"id":"731a18bc","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1159b5dc8c59570993ac39ae349c5037","grade":false,"grade_id":"cell-3fb8ebd5eb97e246","locked":true,"schema_version":3,"solution":false,"task":false},"id":"731a18bc"},"outputs":[],"source":["data = load_diabetes()\n","X_train1, y_train1 = data.data, data.target\n","w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n","b1 = np.random.randn(1) * 0.0001"]},{"cell_type":"code","execution_count":null,"id":"f07dea71","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"baf9b75ec047c1cdb887ecb2bc2043a1","grade":false,"grade_id":"cell-5ffdf99ad7cdfd69","locked":false,"schema_version":3,"solution":true,"task":false},"id":"f07dea71"},"outputs":[],"source":["def mse_loss_naive(w, b, X, y, alpha=0):\n","    \"\"\"\n","    MSE loss function WITH FOR LOOPs\n","\n","    Returns a tuple of:\n","    - loss\n","    - gradient with respect to weights w\n","    - gradient with respect to bias b\n","    \"\"\"\n","\n","    loss = 0.0\n","    dw = np.zeros_like(w)\n","    db = 0.0\n","\n","    # YOUR CODE HERE\n","    size_y = len(y)\n","    y_pred = X.dot(w) + b\n","\n","    # loss\n","    for i in range(size_y):\n","        loss = loss + (y[i] - y_pred[i])**2\n","    loss += alpha * w.T.dot(w)\n","    loss *= (1 / size_y)\n","\n","    # gradient with respect to weights w\n","    for i in range(size_y):\n","        dw += X[i] * (y[i] - y_pred[i])\n","    dw += alpha * w\n","    dw *= (-2 / size_y)\n","\n","    # gradient with respect to weights b\n","    for i in range(size_y):\n","        db += (y[i] - y_pred[i])\n","    db *= (-2 / size_y)\n","\n","    return loss, dw, np.array(db).reshape(1,)"]},{"cell_type":"markdown","id":"bddacc5d","metadata":{"id":"bddacc5d"},"source":["## Naive Linear regression loss"]},{"cell_type":"code","execution_count":null,"id":"86cdcedc","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"67581686e36845f8ef8aacd81c1a201b","grade":true,"grade_id":"cell-079e42b153b67a60","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"86cdcedc","executionInfo":{"status":"ok","timestamp":1702580825916,"user_tz":-180,"elapsed":38,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"9c75b115-0c6b-4d44-da80-9fa6211a9636"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss error :  0.0\n","Gradient check w\n","numerical: -4.145419 analytic: -4.145419, relative error: 5.118220e-08\n","numerical: -4.296089 analytic: -4.296088, relative error: 1.361039e-07\n","numerical: -1.275046 analytic: -1.275045, relative error: 3.422841e-07\n","numerical: -0.315455 analytic: -0.315454, relative error: 9.650044e-07\n","numerical: -3.153319 analytic: -3.153318, relative error: 1.342542e-07\n","numerical: -1.275046 analytic: -1.275045, relative error: 3.422841e-07\n","numerical: -3.153319 analytic: -3.153318, relative error: 1.342542e-07\n","numerical: -1.376395 analytic: -1.376395, relative error: 2.422274e-07\n","numerical: -3.234109 analytic: -3.234110, relative error: 2.105461e-07\n","numerical: -1.553189 analytic: -1.553189, relative error: 2.037040e-07\n","numerical: -1.275046 analytic: -1.275045, relative error: 3.422841e-07\n","numerical: -4.296089 analytic: -4.296088, relative error: 1.361039e-07\n","numerical: -1.275046 analytic: -1.275045, relative error: 3.422841e-07\n","numerical: -1.275046 analytic: -1.275045, relative error: 3.422841e-07\n","numerical: -3.153319 analytic: -3.153318, relative error: 1.342542e-07\n","Gradient check bias\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 2.119851e-10\n"]}],"source":["loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)\n","\n","sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n","assert rel_error(loss, sk_loss) < 1e-9\n","print(\"Loss error : \",rel_error(loss, sk_loss))\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"db438d3f","metadata":{"id":"db438d3f"},"source":["## Naive Ridge regression loss"]},{"cell_type":"code","execution_count":null,"id":"36465817","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9ca5790893e5950341f7370bb3be167c","grade":true,"grade_id":"cell-14a64cb1cfe70b98","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"36465817","executionInfo":{"status":"ok","timestamp":1702580825917,"user_tz":-180,"elapsed":32,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"8fefde25-bc8e-4e32-d2ac-fc7eb7e41146"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: 2.892061 analytic: 2.892060, relative error: 7.955801e-08\n","numerical: -1.553189 analytic: -1.553188, relative error: 3.085007e-07\n","numerical: -2.801915 analytic: -2.801914, relative error: 1.962322e-07\n","numerical: -0.315455 analytic: -0.315455, relative error: 2.500889e-07\n","numerical: -3.153320 analytic: -3.153318, relative error: 2.750109e-07\n","numerical: -1.275046 analytic: -1.275044, relative error: 8.196173e-07\n","numerical: -1.275046 analytic: -1.275044, relative error: 8.196173e-07\n","numerical: -1.553189 analytic: -1.553188, relative error: 3.085007e-07\n","numerical: -1.376396 analytic: -1.376394, relative error: 7.002880e-07\n","numerical: -1.275046 analytic: -1.275044, relative error: 8.196173e-07\n","numerical: -1.275046 analytic: -1.275044, relative error: 8.196173e-07\n","numerical: -2.801915 analytic: -2.801914, relative error: 1.962322e-07\n","numerical: -1.275046 analytic: -1.275044, relative error: 8.196173e-07\n","numerical: -2.801915 analytic: -2.801914, relative error: 1.962322e-07\n","numerical: -4.145419 analytic: -4.145419, relative error: 2.320256e-09\n","Gradient check bias\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 5.108987e-10\n"]}],"source":["loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"]},{"cell_type":"code","execution_count":null,"id":"5c6b011f","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"21b3a09270262dbad9cb1c15c0fc69f8","grade":false,"grade_id":"cell-1528a28f467d90c7","locked":false,"schema_version":3,"solution":true,"task":false},"id":"5c6b011f"},"outputs":[],"source":["def mse_loss_vectorized(w, b, X, y, alpha=0):\n","    \"\"\"\n","    MSE loss function WITHOUT FOR LOOPs\n","\n","    Returns a tuple of:\n","    - loss\n","    - gradient with respect to weights w\n","    - gradient with respect to bias b\n","    \"\"\"\n","    loss = 0.0\n","    dw = np.zeros_like(w)\n","    db = 0.0\n","\n","    # YOUR CODE HERE\n","    y_pred = X.dot(w) + b\n","\n","    # loss\n","    s1 =  (y_pred - y).T @ (y_pred - y) + alpha * (w.T @ w)\n","    loss =  (1/(y.shape[0])) * s1\n","\n","    # gradient with respect to weights w\n","    s1 = alpha * w\n","    s2 = X.T @ (y_pred - y)\n","    dw = (s1 + s2) * (2/y.shape[0])\n","\n","    # gradient with respect to weights b\n","    s3 = np.sum(y_pred - y)\n","    db = s3 * (2/y.shape[0])\n","\n","    return loss, dw, np.array(db).reshape(1,)"]},{"cell_type":"markdown","id":"1d9bff00","metadata":{"id":"1d9bff00"},"source":["## Vectorised Linear regression loss"]},{"cell_type":"code","execution_count":null,"id":"05dd3b4d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ffc6cc1a998e8ed785bea62c0208ce04","grade":true,"grade_id":"cell-41637ca21c8f938d","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"05dd3b4d","executionInfo":{"status":"ok","timestamp":1702580825918,"user_tz":-180,"elapsed":26,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"5c7f8ea8-4ac3-44c8-d374-5e0163afd440"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss error :  1.2512632702993494e-16\n","Gradient check w\n","numerical: -1.376395 analytic: -1.376395, relative error: 8.816271e-08\n","numerical: -4.145419 analytic: -4.145419, relative error: 7.302691e-09\n","numerical: -0.315454 analytic: -0.315454, relative error: 7.648710e-07\n","numerical: -0.315454 analytic: -0.315454, relative error: 7.648710e-07\n","numerical: -1.553188 analytic: -1.553189, relative error: 3.052239e-08\n","numerical: -0.315454 analytic: -0.315454, relative error: 7.648710e-07\n","numerical: 2.892061 analytic: 2.892061, relative error: 5.626271e-08\n","numerical: -3.153318 analytic: -3.153318, relative error: 1.888435e-08\n","numerical: 2.892061 analytic: 2.892061, relative error: 5.626271e-08\n","numerical: -0.315454 analytic: -0.315454, relative error: 7.648710e-07\n","numerical: -0.315454 analytic: -0.315454, relative error: 7.648710e-07\n","numerical: -2.801914 analytic: -2.801914, relative error: 5.239719e-08\n","numerical: -4.296088 analytic: -4.296088, relative error: 9.082135e-09\n","numerical: -3.153318 analytic: -3.153318, relative error: 1.888435e-08\n","numerical: -3.153318 analytic: -3.153318, relative error: 1.888435e-08\n","Gradient check bias\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n"]}],"source":["loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)\n","\n","sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n","assert rel_error(loss, sk_loss) < 1e-9\n","print(\"Loss error : \",rel_error(loss, sk_loss))\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"27c8575f","metadata":{"id":"27c8575f"},"source":["## Vectorized ridge regression loss"]},{"cell_type":"code","execution_count":null,"id":"6aa107f7","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"89b351bee282f05c3f74c9541b3a825e","grade":true,"grade_id":"cell-bb0ff99c19a85e0f","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"6aa107f7","executionInfo":{"status":"ok","timestamp":1702580825918,"user_tz":-180,"elapsed":21,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"382a82bc-bb28-455b-aa5c-6076fea40291"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: -3.153319 analytic: -3.153319, relative error: 2.233997e-08\n","numerical: -3.234110 analytic: -3.234110, relative error: 2.315559e-08\n","numerical: -1.553188 analytic: -1.553189, relative error: 7.676247e-08\n","numerical: -4.145418 analytic: -4.145419, relative error: 9.654628e-09\n","numerical: -4.145418 analytic: -4.145419, relative error: 9.654628e-09\n","numerical: -2.801915 analytic: -2.801914, relative error: 3.840119e-08\n","numerical: -4.296088 analytic: -4.296088, relative error: 4.224163e-09\n","numerical: -4.296088 analytic: -4.296088, relative error: 4.224163e-09\n","numerical: -3.153319 analytic: -3.153319, relative error: 2.233997e-08\n","numerical: -4.145418 analytic: -4.145419, relative error: 9.654628e-09\n","numerical: -1.553188 analytic: -1.553189, relative error: 7.676247e-08\n","numerical: -2.801915 analytic: -2.801914, relative error: 3.840119e-08\n","numerical: -1.275045 analytic: -1.275045, relative error: 2.063795e-07\n","numerical: -1.553188 analytic: -1.553189, relative error: 7.676247e-08\n","numerical: -3.234110 analytic: -3.234110, relative error: 2.315559e-08\n","Gradient check bias\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n","numerical: -304.266734 analytic: -304.266734, relative error: 3.858421e-10\n"]}],"source":["loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n","\n","    in this dimensions\n","    \"\"\"\n","\n","    for i in range(num_checks):\n","        ix = tuple([randrange(m) for m in x.shape])\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"c3c7ae33","metadata":{"id":"c3c7ae33"},"source":["# Logistic regression"]},{"cell_type":"code","execution_count":null,"id":"bb6d1eb4","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fd86b658f3ec113c00c2ee2e195f550f","grade":false,"grade_id":"cell-434ec399cf8aeea7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"bb6d1eb4"},"outputs":[],"source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","data = load_breast_cancer()\n","X_train2, y_train2 = data.data, data.target\n","w2 = np.random.randn(X_train2.shape[1]) * 0.0001\n","b2 = np.random.randn(1) * 0.0001"]},{"cell_type":"markdown","id":"d97e5d16","metadata":{"id":"d97e5d16"},"source":["# Naive"]},{"cell_type":"code","execution_count":null,"id":"392e05fa","metadata":{"id":"392e05fa"},"outputs":[],"source":["def logistic(y, m):\n","    res = y * np.log(m) + (1-y) * np.log(1-m)\n","    return res"]},{"cell_type":"code","execution_count":null,"id":"4690c331","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a4357b941a04e6dc51a2683b7f4d91ed","grade":false,"grade_id":"cell-6ec156568b0c6e29","locked":false,"schema_version":3,"solution":true,"task":false},"id":"4690c331"},"outputs":[],"source":["def log_loss_naive(w, b, X, y, alpha=0):\n","    \"\"\"\n","    log loss function WITH FOR LOOPs\n","\n","    Returns a tuple of:\n","    - loss\n","    - gradient with respect to weights w\n","    \"\"\"\n","    loss = 0.0\n","    dw = np.zeros_like(w)\n","    db = 0.0\n","\n","    # YOUR CODE HERE\n","\n","    # loss\n","    for i in range(y.shape[0]):\n","        sig = sigmoid(w.T.dot(X[i]) + b)\n","        loss -= logistic(y[i], sig)\n","\n","    loss += alpha * w.T.dot(w)\n","    loss = (1/y.shape[0]) * loss\n","\n","    # gradient with respect to weights w\n","    for i in range(w.shape[0]):\n","        s1 = 2 * alpha * w[i]\n","        for j in range(y.shape[0]):\n","            sig = sigmoid((w.T @ X[j]) + b)\n","            s1 += (sig - y[j]) * X[j][i]\n","        dw[i] = s1/y.shape[0]\n","\n","    # gradient with respect to bias b\n","    s2 = 0\n","    for i in range(y.shape[0]):\n","        sig = sigmoid((w.T @ X[i]) + b)\n","        s2 += sig - y[i]\n","    db = s2 * (1/y.shape[0])\n","\n","    return loss, dw, np.array(db).reshape(1,)"]},{"cell_type":"code","execution_count":null,"id":"f54b1bca","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"97d1606d0c00119f8956e4ef60ad346f","grade":true,"grade_id":"cell-d3c078eb2449ee61","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"f54b1bca","executionInfo":{"status":"ok","timestamp":1702580847341,"user_tz":-180,"elapsed":21436,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"ee86c0ec-bb11-4201-e35c-8876cb780829"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss error :  4.0125104342358195e-16\n","Gradient check w\n","numerical: -0.010376 analytic: -0.010376, relative error: 1.247357e-09\n","numerical: -0.646736 analytic: -0.646736, relative error: 1.572244e-10\n","numerical: -0.000405 analytic: -0.000405, relative error: 8.698469e-08\n","numerical: -2.057303 analytic: -2.057303, relative error: 2.221650e-10\n","numerical: -0.008380 analytic: -0.008380, relative error: 1.112005e-09\n","numerical: -2.057303 analytic: -2.057303, relative error: 2.221650e-10\n","numerical: -0.000361 analytic: -0.000361, relative error: 1.349820e-08\n","numerical: 0.021346 analytic: 0.021346, relative error: 3.417589e-10\n","numerical: -0.010376 analytic: -0.010376, relative error: 1.247357e-09\n","numerical: 0.007945 analytic: 0.007945, relative error: 1.414640e-09\n","numerical: 0.021346 analytic: 0.021346, relative error: 3.417589e-10\n","numerical: 0.009822 analytic: 0.009822, relative error: 9.605254e-10\n","numerical: 0.014823 analytic: 0.014823, relative error: 1.395529e-09\n","numerical: -1.703354 analytic: -1.703354, relative error: 1.266375e-10\n","numerical: -0.010376 analytic: -0.010376, relative error: 1.247357e-09\n","Gradient check bias\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n"]}],"source":["y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n","y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n","sk_loss = log_loss(y_train2, y_pred)\n","\n","loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)\n","assert rel_error(loss, sk_loss) < 1e-9\n","print(\"Loss error : \",rel_error(loss, sk_loss))\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"5e1e8516","metadata":{"id":"5e1e8516"},"source":["# Naive with regulariztion"]},{"cell_type":"code","execution_count":null,"id":"79f6f9a6","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"cb3c9584e2ca3502e5068d5c1ee2df96","grade":true,"grade_id":"cell-b91805308ad91ec5","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"79f6f9a6","executionInfo":{"status":"ok","timestamp":1702580879825,"user_tz":-180,"elapsed":32504,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"c2b6c723-1837-4682-8f74-33a0ade27c18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: 0.014823 analytic: 0.014823, relative error: 1.424336e-09\n","numerical: -2.057303 analytic: -2.057303, relative error: 2.233022e-10\n","numerical: -0.000404 analytic: -0.000404, relative error: 7.878113e-08\n","numerical: -0.163838 analytic: -0.163838, relative error: 9.669251e-11\n","numerical: -0.163838 analytic: -0.163838, relative error: 9.669251e-11\n","numerical: -0.364052 analytic: -0.364052, relative error: 5.654486e-10\n","numerical: -1.703354 analytic: -1.703354, relative error: 1.273177e-10\n","numerical: -0.646736 analytic: -0.646736, relative error: 1.557427e-10\n","numerical: -0.010376 analytic: -0.010376, relative error: 1.316386e-09\n","numerical: -2.057303 analytic: -2.057303, relative error: 2.233022e-10\n","numerical: -1.649472 analytic: -1.649473, relative error: 3.665622e-08\n","numerical: 0.014823 analytic: 0.014823, relative error: 1.424336e-09\n","numerical: -0.000404 analytic: -0.000404, relative error: 7.878113e-08\n","numerical: 0.021345 analytic: 0.021345, relative error: 2.868659e-10\n","numerical: 0.014823 analytic: 0.014823, relative error: 1.424336e-09\n","Gradient check bias\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n"]}],"source":["loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"31564092","metadata":{"id":"31564092"},"source":["# Vectorized"]},{"cell_type":"code","execution_count":null,"id":"fc5084e9","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6f452aaa24d69e2257c0851d750fd4aa","grade":false,"grade_id":"cell-a96e9a6d51919ffc","locked":false,"schema_version":3,"solution":true,"task":false},"id":"fc5084e9"},"outputs":[],"source":["def log_loss_vectorized(w, b,X, y, alpha=0):\n","    \"\"\"\n","    log loss function WITHOUT FOR LOOPs\n","\n","    Returns a tuple of:\n","    - loss\n","    - gradient with respect to weights w\n","    \"\"\"\n","    loss = 0.0\n","    dw = np.zeros_like(w)\n","    db = 0.0\n","\n","    # YOUR CODE HERE\n","    # loss\n","    sig = sigmoid(X.dot(w)+b)\n","    loss -= sum( y * np.log(sig) + (1-y) * np.log(1-sig) )\n","    s1 = alpha * w.T.dot(w)\n","    loss = (loss + s1) / y.shape[0]\n","\n","    # gradient with respect to weights w\n","    s1 = 2 * alpha * w\n","    s2 = X.T.dot(sig - y)\n","    dw = (s1 + s2)/y.shape[0]\n","\n","    # gradient with respect to bias b\n","    s3 = sum(sig - y)\n","    db = s3 / y.shape[0]\n","\n","    return loss, dw, np.array(db).reshape(1,)"]},{"cell_type":"code","execution_count":null,"id":"c425e768","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"37c0df2fc2b67514a7bd11e120562fa1","grade":true,"grade_id":"cell-ca14e49ada130789","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"c425e768","executionInfo":{"status":"ok","timestamp":1702580879828,"user_tz":-180,"elapsed":33,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"20102ba9-7c70-46fa-cce3-76a7732fb1b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss error :  3.210008347388655e-16\n","Gradient check w\n","numerical: -1.703354 analytic: -1.703354, relative error: 1.282674e-10\n","numerical: -3.588433 analytic: -3.588433, relative error: 9.748116e-09\n","numerical: -0.163838 analytic: -0.163838, relative error: 7.738657e-11\n","numerical: 0.010976 analytic: 0.010976, relative error: 1.048020e-09\n","numerical: 0.021346 analytic: 0.021346, relative error: 3.417581e-10\n","numerical: 0.009822 analytic: 0.009822, relative error: 9.605254e-10\n","numerical: -0.000867 analytic: -0.000867, relative error: 5.226022e-09\n","numerical: -0.000405 analytic: -0.000405, relative error: 8.012872e-08\n","numerical: -0.019708 analytic: -0.019708, relative error: 3.571989e-10\n","numerical: 0.014823 analytic: 0.014823, relative error: 1.582774e-09\n","numerical: -0.008380 analytic: -0.008380, relative error: 1.112005e-09\n","numerical: -0.019708 analytic: -0.019708, relative error: 3.571989e-10\n","numerical: 0.021346 analytic: 0.021346, relative error: 3.417581e-10\n","numerical: 6.546044 analytic: 6.546044, relative error: 1.416404e-08\n","numerical: 0.029993 analytic: 0.029993, relative error: 1.938305e-10\n","Gradient check bias\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n","numerical: -0.132868 analytic: -0.132868, relative error: 1.446012e-11\n"]}],"source":["y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n","y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n","sk_loss = log_loss(y_train2, y_pred)\n","\n","loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)\n","assert rel_error(loss, sk_loss) < 1e-9\n","print(\"Loss error : \",rel_error(loss, sk_loss))\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"dc7be5da","metadata":{"id":"dc7be5da"},"source":["# Vectorized with regularization"]},{"cell_type":"code","execution_count":null,"id":"a4e09127","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"53e97adc2666d529eff9ba1abcd3756a","grade":true,"grade_id":"cell-bce082fccf8b7057","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"a4e09127","executionInfo":{"status":"ok","timestamp":1702580879829,"user_tz":-180,"elapsed":28,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"7678c162-569c-49db-e53f-81c9fa5da193"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: 0.007945 analytic: 0.007945, relative error: 6.964529e-10\n","numerical: 0.156493 analytic: 0.156493, relative error: 8.214316e-11\n","numerical: -1.649472 analytic: -1.649473, relative error: 3.665622e-08\n","numerical: 0.021345 analytic: 0.021345, relative error: 2.868651e-10\n","numerical: -0.163838 analytic: -0.163838, relative error: 7.975107e-11\n","numerical: -0.001032 analytic: -0.001032, relative error: 4.649319e-09\n","numerical: -3.588432 analytic: -3.588432, relative error: 9.748412e-09\n","numerical: -0.026106 analytic: -0.026106, relative error: 8.136052e-10\n","numerical: -0.000404 analytic: -0.000404, relative error: 8.564436e-08\n","numerical: 6.546044 analytic: 6.546044, relative error: 1.416425e-08\n","numerical: -0.163838 analytic: -0.163838, relative error: 7.975107e-11\n","numerical: -0.019707 analytic: -0.019707, relative error: 3.651731e-10\n","numerical: -0.163838 analytic: -0.163838, relative error: 7.975107e-11\n","numerical: -0.000866 analytic: -0.000866, relative error: 4.650089e-09\n","numerical: -0.010376 analytic: -0.010376, relative error: 1.583886e-09\n","Gradient check bias\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n","numerical: -0.132868 analytic: -0.132868, relative error: 6.429515e-12\n"]}],"source":["loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"]},{"cell_type":"markdown","id":"06403ea0","metadata":{"id":"06403ea0"},"source":["# Gradient descent for Linear models"]},{"cell_type":"code","execution_count":null,"id":"5039af1c","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9a1783db1a1f1898028260238567bed7","grade":false,"grade_id":"cell-485e52c0efd4f4a9","locked":false,"schema_version":3,"solution":true,"task":false},"id":"5039af1c"},"outputs":[],"source":["class LinearModel():\n","    def __init__(self):\n","        self.w = None\n","        self.b = None\n","\n","    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n","        N, d = X.shape\n","\n","        if self.w is None: # Initialization\n","            self.w = 0.001 * np.random.randn(d)\n","            self.b = 0.0\n","\n","        # Run stochastic gradient descent to optimize w\n","\n","        loss_history = []\n","        for it in range(num_iters):\n","            X_batch = None\n","            y_batch = None\n","\n","            # Sample batch_size elements in X_batch and y_batch\n","            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)\n","            # Hint: Use np.random.choice to generate indices\n","            rand = np.random.choice(N, batch_size, replace=False)\n","\n","            X_batch = X[rand, :]\n","            y_batch = y[rand]\n","\n","            # evaluate loss and gradient\n","            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            # Update the weights w and bias b using the gradient and the learning rate.\n","            self.w -= learning_rate * dw\n","            self.b -= learning_rate * db\n","\n","            if verbose and it % 10000 == 0:\n","                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n","\n","        return loss_history\n","\n","    def predict(self, X):\n","        pass\n","\n","    def loss(self, X_batch, y_batch, reg):\n","        pass\n","\n","class LinearRegressor(LinearModel):\n","    \"\"\" Linear regression \"\"\"\n","\n","    def loss(self, X_batch, y_batch, alpha):\n","        return mse_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n","\n","    def predict(self, X):\n","        y = X.dot(self.w) + self.b\n","        return y\n","\n","class LogisticRegressor(LinearModel):\n","    \"\"\" Linear regression \"\"\"\n","\n","    def loss(self, X_batch, y_batch, alpha):\n","        return log_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n","\n","    def predict(self, X):\n","        \"\"\" Return prediction labels vector of 0 or 1 \"\"\"\n","        # YOUR CODE HERE\n","        y = X.dot(self.w) + self.b\n","        r = np.zeros_like(y)\n","        for i in range(len(y)):\n","            r[i] = 1 if y[i]>0 else 0\n","        return r"]},{"cell_type":"markdown","id":"7fa0aa79","metadata":{"id":"7fa0aa79"},"source":["## Linear regression with gradient descent"]},{"cell_type":"code","execution_count":null,"id":"86925e00","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"30f1c271b16efe670214663c10743937","grade":true,"grade_id":"cell-92f36a3b387a4277","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"86925e00","executionInfo":{"status":"ok","timestamp":1702580882121,"user_tz":-180,"elapsed":2314,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"031b66d8-7bd7-4186-c0c6-614e3e93faf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration 0 / 75000: loss 25977.313070\n","iteration 10000 / 75000: loss 3818.439766\n","iteration 20000 / 75000: loss 2990.781528\n","iteration 30000 / 75000: loss 2259.751556\n","iteration 40000 / 75000: loss 3564.815899\n","iteration 50000 / 75000: loss 3599.266503\n","iteration 60000 / 75000: loss 3402.548861\n","iteration 70000 / 75000: loss 2694.861284\n","MSE scikit-learn: 2859.69634758675\n","MSE gradient descent model : 2884.4364378534956\n"]}],"source":["from sklearn.linear_model import LinearRegression\n","\n","sk_model = LinearRegression(fit_intercept=True)\n","sk_model.fit(X_train1, y_train1)\n","sk_pred = sk_model.predict(X_train1)\n","sk_mse = mean_squared_error(sk_pred, y_train1)\n","\n","model = LinearRegressor()\n","model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n","pred = model.predict(X_train1)\n","mse = mean_squared_error(pred, y_train1)\n","\n","print(\"MSE scikit-learn:\", sk_mse)\n","print(\"MSE gradient descent model :\", mse)\n","assert mse - sk_mse < 100"]},{"cell_type":"markdown","id":"d17c54d8","metadata":{"id":"d17c54d8"},"source":["## Logistc regression with gradient descent"]},{"cell_type":"code","execution_count":null,"id":"9219f422","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c6196155f76f4eded31fefce67a76d60","grade":true,"grade_id":"cell-925a2ddb5c1ba7ff","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"9219f422","executionInfo":{"status":"ok","timestamp":1702580894658,"user_tz":-180,"elapsed":12550,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"a31c8859-811c-4aa4-e390-1599680579dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration 0 / 75000: loss 0.694113\n","iteration 10000 / 75000: loss 0.088341\n","iteration 20000 / 75000: loss 0.030155\n","iteration 30000 / 75000: loss 0.138437\n","iteration 40000 / 75000: loss 0.037893\n","iteration 50000 / 75000: loss 0.051293\n","iteration 60000 / 75000: loss 0.039052\n","iteration 70000 / 75000: loss 0.097594\n","Log-loss scikit-learn: 0.44341928598210933\n","Log-loss gradiet descent model : 0.44341928598210933\n","Error : 0.0\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing\n","    in this dimensions\n","    \"\"\"\n","\n","    for i in range(num_checks):\n","        ix = tuple([randrange(m) for m in x.shape])\n","import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train2 = scaler.fit_trans\n","    in this dimensions\n","    \"\"\"\n","\n","    for i in range(num_checks):\n","        ix = tuple([randrange(m) for m in x.shape])\n","form(X_train2)\n","\n","sk_model = LogisticRegression(fit_intercept=True)\n","sk_model.fit(X_train2, y_train2)\n","sk_pred = sk_model.predict(X_train2)\n","sk_log_loss = log_loss(sk_pred, y_train2)\n","\n","model = LogisticRegressor()\n","model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n","pred = model.predict(X_train2)\n","model_log_loss = log_loss(pred, y_train2)\n","\n","print(\"Log-loss scikit-learn:\", sk_log_loss)\n","print(\"Log-loss gradiet descent model :\", model_log_loss)\n","print(\"Error :\", rel_error(sk_log_loss, model_log_loss))\n","assert rel_error(sk_log_loss, model_log_loss) < 1e-7"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}