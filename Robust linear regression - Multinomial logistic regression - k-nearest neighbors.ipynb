{"cells":[{"cell_type":"markdown","id":"83174cc5","metadata":{"id":"83174cc5"},"source":["Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n","\n","Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"]},{"cell_type":"code","source":["pip install scikit-learn==1.1.3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tufRZXBF6u_f","executionInfo":{"status":"ok","timestamp":1702987874474,"user_tz":-60,"elapsed":24540,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"outputId":"faadc208-9a35-4866-9f93-6af8009ef2b4"},"id":"tufRZXBF6u_f","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==1.1.3\n","  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.11.4)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.16.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"]}]},{"cell_type":"markdown","id":"ad8c49f5","metadata":{"id":"ad8c49f5"},"source":["---"]},{"cell_type":"code","execution_count":2,"id":"aebdce2a","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a003bb8fd03260393cb0121b2dcaf283","grade":false,"grade_id":"cell-bb34820fb5daebb2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"aebdce2a","executionInfo":{"status":"ok","timestamp":1702987876284,"user_tz":-60,"elapsed":1813,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["# AZA MANAMPY CODE ATO FA MNAOVA CELLULE VAOVAO\n","\n","from random import randrange\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, log_loss\n","from sklearn.linear_model import HuberRegressor\n","from sklearn.datasets import load_boston, load_diabetes, load_iris, load_digits\n","from scipy.special import huber\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","def grad_check_sparse(f, x, analytic_grad, num_checks=12, h=1e-5, error=1e-9):\n","    \"\"\"\n","    sample a few random elements and only return numerical\n","    in this dimensions\n","    \"\"\"\n","\n","    for i in range(num_checks):\n","        ix = tuple([randrange(m) for m in x.shape])\n","\n","        oldval = x[ix]\n","        x[ix] = oldval + h  # increment by h\n","        fxph = f(x)  # evaluate f(x + h)\n","        x[ix] = oldval - h  # increment by h\n","        fxmh = f(x)  # evaluate f(x - h)\n","        x[ix] = oldval  # reset\n","\n","        grad_numerical = (fxph - fxmh) / (2 * h)\n","        grad_analytic = analytic_grad[ix]\n","        rel_error = abs(grad_numerical - grad_analytic) / (\n","            abs(grad_numerical) + abs(grad_analytic)\n","        )\n","        print(\n","            \"numerical: %f analytic: %f, relative error: %e\"\n","            % (grad_numerical, grad_analytic, rel_error)\n","        )\n","        assert rel_error < error\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","id":"b72ea596","metadata":{"id":"b72ea596"},"source":["# Robust linear regression - Huber loss"]},{"cell_type":"code","execution_count":3,"id":"9405baa1","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c1ef69a259cc98ed9c1a4b75d4675c8d","grade":false,"grade_id":"cell-050726493bce8a41","locked":true,"schema_version":3,"solution":false,"task":false},"id":"9405baa1","executionInfo":{"status":"ok","timestamp":1702987876284,"user_tz":-60,"elapsed":10,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fe27bdf-34d9-4a69-cf43-3778a431d99c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["data = load_boston()\n","X_train1, y_train1 = data.data, data.target\n","w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n","b1 = np.random.randn(1) * 0.0001"]},{"cell_type":"code","execution_count":4,"id":"b79b3750","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"095fe70b6ce7c4be3afef42fd9756ac5","grade":false,"grade_id":"cell-773a9a9798718bb2","locked":false,"schema_version":3,"solution":true,"task":false},"id":"b79b3750","executionInfo":{"status":"ok","timestamp":1702987876285,"user_tz":-60,"elapsed":7,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def huber_loss_naive(w, b, X, y, epsilon=1.35, alpha=0.0001):\n","    \"\"\"\n","    Huber loss for all observations\n","\n","    Inputs:\n","    - w: array of shape (D,) containing weights\n","    - b: float bias\n","    - X: array of shape (N, D) containing a minibatch of data\n","    - y: array of shape (N,) containing training labels\n","    - epsilon: float\n","    - alpha: regularization\n","    \"\"\"\n","    loss = 0.0\n","    dw = np.zeros_like(w)\n","    db = 0.0\n","\n","    # YOUR CODE HERE\n","    # loss\n","    for i in range(y.shape[0]):\n","        r = sum(X[i].dot(w) + b - y[i]) # array -> float\n","        if abs(r) <= epsilon:\n","            loss += (r**2 / 2)\n","        else:\n","            loss += (epsilon * abs(r) - epsilon**2/2)\n","    loss *= (1/y.shape[0])\n","    for j in range(w.shape[0]):\n","        loss += alpha * w[j]**2\n","\n","    # dw\n","    for i in range(w.shape[0]):\n","        alph = 2 * alpha * w[i]\n","        s1 = 0\n","        for j in range(y.shape[0]):\n","            r = sum(w.T.dot(X[j]) + b - y[j])\n","            if abs(r) <= epsilon:\n","                s1 += X[j][i] * r\n","            else:\n","                s1 += np.sign(r) * X[j][i] * epsilon\n","        dw[i] = (alph + (s1 / y.shape[0]))\n","\n","    # db\n","    for j in range(y.shape[0]):\n","        r = sum(X[j].dot(w) + b - y[j])\n","        if abs(r) <= epsilon:\n","            db += r\n","        else:\n","            db += np.sign(r) * epsilon\n","    db *= 1/y.shape[0]\n","\n","    return loss, dw, np.array(db).reshape(1,)"]},{"cell_type":"markdown","id":"92ca4921","metadata":{"id":"92ca4921"},"source":["## without regularization"]},{"cell_type":"code","execution_count":5,"id":"7ae434b9","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2e49c0c5b9c763c872c9eccf3c34e14f","grade":true,"grade_id":"cell-88996bb95da97f10","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"7ae434b9","executionInfo":{"status":"ok","timestamp":1702987884020,"user_tz":-60,"elapsed":7742,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"703ba0b5-ee0d-46ee-d0e4-395872f1291c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: -17.081635 analytic: -17.081635, relative error: 1.069986e-11\n","numerical: -0.748838 analytic: -0.748838, relative error: 3.370299e-10\n","numerical: -481.509943 analytic: -481.509943, relative error: 9.850295e-13\n","numerical: -481.509943 analytic: -481.509943, relative error: 9.850295e-13\n","numerical: -551.120158 analytic: -551.120158, relative error: 9.687060e-13\n","numerical: -15.034651 analytic: -15.034651, relative error: 2.154061e-11\n","numerical: -24.914970 analytic: -24.914970, relative error: 1.968967e-11\n","numerical: -92.576117 analytic: -92.576117, relative error: 1.353219e-12\n","numerical: -24.914970 analytic: -24.914970, relative error: 1.968967e-11\n","numerical: -5.123308 analytic: -5.123308, relative error: 1.798891e-11\n","numerical: -0.748838 analytic: -0.748838, relative error: 3.370299e-10\n","numerical: -5.123308 analytic: -5.123308, relative error: 1.798891e-11\n","numerical: -5.123308 analytic: -5.123308, relative error: 1.798891e-11\n","numerical: -0.748838 analytic: -0.748838, relative error: 3.370299e-10\n","numerical: -4.878257 analytic: -4.878257, relative error: 1.315611e-11\n","Gradient check bias\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","Gradient check w large epsilon\n","numerical: -1.975020 analytic: -1.975020, relative error: 4.660006e-10\n","numerical: -51.274742 analytic: -51.274742, relative error: 1.008264e-10\n","numerical: -8356.683679 analytic: -8356.683679, relative error: 2.214780e-13\n","numerical: -238.274569 analytic: -238.274569, relative error: 3.665999e-11\n","numerical: -8523.948247 analytic: -8523.948247, relative error: 1.138476e-13\n","numerical: -1.975020 analytic: -1.975020, relative error: 4.660006e-10\n","numerical: -8356.683679 analytic: -8356.683679, relative error: 2.214780e-13\n","numerical: -334.378941 analytic: -334.378941, relative error: 4.634033e-12\n","numerical: -8356.683679 analytic: -8356.683679, relative error: 2.214780e-13\n","numerical: -185.928313 analytic: -185.928313, relative error: 1.001689e-10\n","numerical: -1455.930330 analytic: -1455.930330, relative error: 2.384336e-12\n","numerical: -146.804997 analytic: -146.804997, relative error: 3.036489e-11\n","numerical: -90.750699 analytic: -90.750699, relative error: 1.236976e-10\n","numerical: -407.882776 analytic: -407.882776, relative error: 1.926737e-11\n","numerical: -51.274742 analytic: -51.274742, relative error: 1.008264e-10\n","Gradient check bias large epsilon\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n"]}],"source":["loss, dw1, db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n","\n","\n","# Large epsilon\n","large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n","\n","print(\"Gradient check w large epsilon\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias large epsilon\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"]},{"cell_type":"markdown","id":"9f2b3314","metadata":{"id":"9f2b3314"},"source":[" ## with regularization"]},{"cell_type":"code","execution_count":6,"id":"20ed0b8d","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"42c6c09b6a3764a91aa10791fd05545e","grade":true,"grade_id":"cell-6cfedcc6f02a9770","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"20ed0b8d","executionInfo":{"status":"ok","timestamp":1702987898671,"user_tz":-60,"elapsed":14658,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c88b78c-881c-48e6-be07-27fcb46b7385"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: -12.891960 analytic: -12.891960, relative error: 3.956609e-11\n","numerical: -551.120445 analytic: -551.120445, relative error: 9.060985e-13\n","numerical: -5.123411 analytic: -5.123411, relative error: 2.047249e-11\n","numerical: -15.341157 analytic: -15.341157, relative error: 3.589111e-11\n","numerical: -0.748445 analytic: -0.748445, relative error: 3.106978e-10\n","numerical: -0.093691 analytic: -0.093691, relative error: 3.720232e-10\n","numerical: -92.576433 analytic: -92.576433, relative error: 1.020571e-12\n","numerical: -4.878205 analytic: -4.878205, relative error: 1.153218e-11\n","numerical: -24.915198 analytic: -24.915198, relative error: 1.727483e-11\n","numerical: -4.878205 analytic: -4.878205, relative error: 1.153218e-11\n","numerical: -15.341157 analytic: -15.341157, relative error: 3.589111e-11\n","numerical: -0.748445 analytic: -0.748445, relative error: 3.106978e-10\n","numerical: -5.123411 analytic: -5.123411, relative error: 2.047249e-11\n","numerical: -12.891960 analytic: -12.891960, relative error: 3.956609e-11\n","numerical: -5.123411 analytic: -5.123411, relative error: 2.047249e-11\n","Gradient check bias\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.097274e-10\n","Gradient check w large epsilon\n","numerical: -334.379189 analytic: -334.379189, relative error: 3.698877e-12\n","numerical: -8356.683898 analytic: -8356.683898, relative error: 2.424830e-13\n","numerical: -238.274588 analytic: -238.274588, relative error: 3.534389e-11\n","numerical: -51.274690 analytic: -51.274690, relative error: 9.027883e-11\n","numerical: -146.805340 analytic: -146.805340, relative error: 3.435678e-11\n","numerical: -407.883004 analytic: -407.883004, relative error: 1.911985e-11\n","numerical: -1455.930646 analytic: -1455.930646, relative error: 2.466482e-12\n","numerical: -407.883004 analytic: -407.883004, relative error: 1.911985e-11\n","numerical: -185.928573 analytic: -185.928573, relative error: 1.069236e-10\n","numerical: -185.928573 analytic: -185.928573, relative error: 1.069236e-10\n","numerical: -1455.930646 analytic: -1455.930646, relative error: 2.466482e-12\n","numerical: -8523.948534 analytic: -8523.948534, relative error: 1.803209e-13\n","numerical: -1.975332 analytic: -1.975332, relative error: 6.349527e-10\n","numerical: -8356.683898 analytic: -8356.683898, relative error: 2.424830e-13\n","numerical: -51.274690 analytic: -51.274690, relative error: 9.027883e-11\n","Gradient check bias large epsilon\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n"]}],"source":["loss, dw1, db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n","\n","\n","# Large epsilon\n","large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n","\n","print(\"Gradient check w large epsilon\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias large epsilon\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"]},{"cell_type":"code","execution_count":7,"id":"038cf336","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6809cf643812e18004a8e019e8648ee7","grade":false,"grade_id":"cell-2f0ecc6405bdde00","locked":false,"schema_version":3,"solution":true,"task":false},"id":"038cf336","executionInfo":{"status":"ok","timestamp":1702987898671,"user_tz":-60,"elapsed":11,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def huber_loss_vectorized(w, b, X, y, epsilon=1.35, alpha=0.0001):\n","    \"\"\"\n","    Huber loss for all observations\n","\n","    Inputs:\n","    - w: array of shape (D,) containing weights\n","    - b: float bias\n","    - X: array of shape (N, D) containing a minibatch of data\n","    - y: array of shape (N,) containing training labels\n","    - epsilon: float\n","    - alpha: regularization\n","    \"\"\"\n","\n","    loss = 0.0\n","    dw = np.zeros_like(w)\n","    db = 0\n","\n","    # YOUR CODE HERE\n","    r = X.dot(w) + b - y\n","    size_y = y.shape[0]\n","\n","    # array for |r| <= e, else 0\n","    eps_inf = np.copy(r)\n","    eps_inf[abs(r)>epsilon] = 0\n","\n","    # array for |r| > e, else 0\n","    eps_sup = np.copy(r)\n","    eps_sup[abs(r)<=epsilon] = 0\n","\n","    # loss\n","    s1 = eps_inf * (eps_inf) / 2\n","    s2 = epsilon * (np.abs(eps_sup) - epsilon/2)\n","    s2[abs(r)<=epsilon] = 0\n","    loss = (1/size_y) * sum(s1 + s2) + alpha * w.T.dot(w)\n","\n","    # dw\n","    s1 = X.T.dot(eps_inf)\n","    s2 = epsilon * X.T.dot(np.sign(eps_sup))\n","    dw = (1/size_y) * (s1 + s2) + 2 * alpha * w\n","\n","    # db\n","    s1 = eps_inf\n","    s2 = epsilon * np.sign(eps_sup)\n","    db = (1/size_y) * sum(s1 + s2)\n","\n","    return loss, dw, np.array(db).reshape(1,)"]},{"cell_type":"markdown","id":"292f866a","metadata":{"id":"292f866a"},"source":["## without regularization"]},{"cell_type":"code","execution_count":8,"id":"ff23bae6","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"cadca09dc4ee08cf4c73480cfdc14e13","grade":true,"grade_id":"cell-cecbe864e0fa149c","locked":true,"points":1.5,"schema_version":3,"solution":false,"task":false},"id":"ff23bae6","executionInfo":{"status":"ok","timestamp":1702987899346,"user_tz":-60,"elapsed":685,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95b2055c-9805-47f8-d34e-58cf40f4b15a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: -0.748838 analytic: -0.748838, relative error: 3.370301e-10\n","numerical: -0.093379 analytic: -0.093379, relative error: 6.023261e-10\n","numerical: -4.878257 analytic: -4.878257, relative error: 1.315584e-11\n","numerical: -15.340909 analytic: -15.340909, relative error: 3.287663e-11\n","numerical: -0.093379 analytic: -0.093379, relative error: 6.023261e-10\n","numerical: -24.914970 analytic: -24.914970, relative error: 2.325216e-11\n","numerical: -24.914970 analytic: -24.914970, relative error: 2.325216e-11\n","numerical: -5.123308 analytic: -5.123308, relative error: 3.532451e-11\n","numerical: -5.123308 analytic: -5.123308, relative error: 3.532451e-11\n","numerical: -92.576117 analytic: -92.576117, relative error: 1.352682e-12\n","numerical: -15.340909 analytic: -15.340909, relative error: 3.287663e-11\n","numerical: -12.891700 analytic: -12.891700, relative error: 4.053349e-11\n","numerical: -481.509943 analytic: -481.509943, relative error: 9.855017e-13\n","numerical: -12.891700 analytic: -12.891700, relative error: 4.053349e-11\n","numerical: -15.034651 analytic: -15.034651, relative error: 2.154416e-11\n","Gradient check bias\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","Gradient check w large epsilon\n","numerical: -1.975020 analytic: -1.975020, relative error: 4.660006e-10\n","numerical: -238.274569 analytic: -238.274569, relative error: 3.665999e-11\n","numerical: -90.750699 analytic: -90.750699, relative error: 1.393568e-10\n","numerical: -407.882776 analytic: -407.882776, relative error: 1.926737e-11\n","numerical: -90.750699 analytic: -90.750699, relative error: 1.393568e-10\n","numerical: -8356.683679 analytic: -8356.683679, relative error: 2.214780e-13\n","numerical: -51.274742 analytic: -51.274742, relative error: 1.008264e-10\n","numerical: -407.882776 analytic: -407.882776, relative error: 1.926737e-11\n","numerical: -1455.930330 analytic: -1455.930330, relative error: 2.384336e-12\n","numerical: -146.804997 analytic: -146.804997, relative error: 3.036489e-11\n","numerical: -185.928313 analytic: -185.928313, relative error: 1.001689e-10\n","numerical: -90.750699 analytic: -90.750699, relative error: 1.393568e-10\n","numerical: -407.882776 analytic: -407.882776, relative error: 1.926737e-11\n","numerical: -8523.948247 analytic: -8523.948247, relative error: 1.138476e-13\n","numerical: -334.378941 analytic: -334.378941, relative error: 4.634033e-12\n","Gradient check bias large epsilon\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n"]}],"source":["loss, dw1, db1 = huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n","\n","\n","# Large epsilon\n","large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)\n","\n","print(\"Gradient check w large epsilon\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias large epsilon\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=0)[0]\n","grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"]},{"cell_type":"markdown","id":"5d367b1c","metadata":{"id":"5d367b1c"},"source":["## with regularization"]},{"cell_type":"code","execution_count":9,"id":"ee9c47a5","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"353c4a3ff31d5ae93f310db8aada6b16","grade":true,"grade_id":"cell-fc0f22937553dd38","locked":true,"points":1.5,"schema_version":3,"solution":false,"task":false},"id":"ee9c47a5","executionInfo":{"status":"ok","timestamp":1702987899346,"user_tz":-60,"elapsed":5,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"540ba378-5ad8-4dfd-efab-0a2e9cfb7326"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check w\n","numerical: -24.915198 analytic: -24.915198, relative error: 2.440209e-11\n","numerical: -481.510162 analytic: -481.510162, relative error: 9.912268e-13\n","numerical: -0.748445 analytic: -0.748445, relative error: 3.106980e-10\n","numerical: -0.748445 analytic: -0.748445, relative error: 3.106980e-10\n","numerical: -17.081654 analytic: -17.081654, relative error: 8.259467e-12\n","numerical: -15.034513 analytic: -15.034513, relative error: 2.381252e-11\n","numerical: -17.081654 analytic: -17.081654, relative error: 8.259467e-12\n","numerical: -12.891960 analytic: -12.891960, relative error: 3.956485e-11\n","numerical: -17.081654 analytic: -17.081654, relative error: 8.259467e-12\n","numerical: -15.034513 analytic: -15.034513, relative error: 2.381252e-11\n","numerical: -4.878205 analytic: -4.878205, relative error: 1.153191e-11\n","numerical: -0.093691 analytic: -0.093691, relative error: 3.720236e-10\n","numerical: -15.034513 analytic: -15.034513, relative error: 2.381252e-11\n","numerical: -5.123411 analytic: -5.123411, relative error: 3.780774e-11\n","numerical: -12.891960 analytic: -12.891960, relative error: 3.956485e-11\n","Gradient check bias\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","numerical: -1.350000 analytic: -1.350000, relative error: 2.755184e-10\n","Gradient check w large epsilon\n","numerical: -146.805340 analytic: -146.805340, relative error: 2.467672e-11\n","numerical: -8356.683898 analytic: -8356.683898, relative error: 7.237488e-14\n","numerical: -51.274690 analytic: -51.274690, relative error: 9.027883e-11\n","numerical: -8356.683898 analytic: -8356.683898, relative error: 7.237488e-14\n","numerical: -1455.930646 analytic: -1455.930646, relative error: 1.490415e-12\n","numerical: -1.975332 analytic: -1.975332, relative error: 6.349527e-10\n","numerical: -334.379189 analytic: -334.379189, relative error: 3.698877e-12\n","numerical: -1.975332 analytic: -1.975332, relative error: 6.349527e-10\n","numerical: -407.883004 analytic: -407.883004, relative error: 1.911985e-11\n","numerical: -221.852423 analytic: -221.852423, relative error: 4.535125e-14\n","numerical: -1.975332 analytic: -1.975332, relative error: 6.349527e-10\n","numerical: -185.928573 analytic: -185.928573, relative error: 1.069236e-10\n","numerical: -1.975332 analytic: -1.975332, relative error: 6.349527e-10\n","numerical: -90.750802 analytic: -90.750802, relative error: 1.394969e-10\n","numerical: -8356.683898 analytic: -8356.683898, relative error: 7.237488e-14\n","Gradient check bias large epsilon\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n","numerical: -22.646439 analytic: -22.646439, relative error: 6.114076e-11\n"]}],"source":["loss, dw1, db1 = huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)\n","\n","print(\"Gradient check w\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w1, dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=1.35, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b1, db1, 15, error=1e-9)\n","\n","\n","# Large epsilon\n","large_eps_loss, large_eps_dw1, large_eps_db1 = huber_loss_naive(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)\n","\n","print(\"Gradient check w large epsilon\")\n","# Check with numerical gradient w\n","f = lambda w1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f, w1, large_eps_dw1, 15, error=1e-9)\n","\n","print(\"Gradient check bias large epsilon\")\n","# Check with numerical gradient b\n","f2 = lambda b1: huber_loss_vectorized(w1, b1, X_train1, y_train1, epsilon=135, alpha=1)[0]\n","grad_numerical = grad_check_sparse(f2, b1, large_eps_db1, 15, error=1e-9)"]},{"cell_type":"code","execution_count":10,"id":"ed260f95","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"51a2209d0893d2b94c64bc684159f81c","grade":false,"grade_id":"cell-9bc0bf420f2797ba","locked":false,"schema_version":3,"solution":true,"task":false},"id":"ed260f95","executionInfo":{"status":"ok","timestamp":1702987899346,"user_tz":-60,"elapsed":3,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["class LinearModel():\n","    def __init__(self):\n","        self.w = None\n","        self.b = None\n","\n","    def train(self, X, y, learning_rate=1e-3, alpha=0.0001, num_iters=100, batch_size=200, verbose=False):\n","        N, d = X.shape\n","\n","        if self.w is None: # Initialization\n","            self.w = 0.001 * np.random.randn(d)\n","            self.b = 0.0\n","\n","        # Run stochastic gradient descent to optimize w\n","\n","        loss_history = []\n","        for it in range(num_iters):\n","            X_batch = None\n","            y_batch = None\n","\n","            # Sample batch_size elements in X_batch and y_batch\n","            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)\n","            # Hint: Use np.random.choice to generate indices\n","            # YOUR CODE HERE\n","            rand = np.random.choice(N, batch_size, replace=False)\n","\n","            X_batch = X[rand, :]\n","            y_batch = y[rand]\n","\n","            # evaluate loss and gradient\n","            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            # Update the weights w and bias b using the gradient and the learning rate.\n","            # YOUR CODE HERE\n","            self.w -= learning_rate * dw\n","            self.b -= learning_rate * db\n","\n","            if verbose and it % 10000 == 0:\n","                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n","\n","        return loss_history\n","\n","    def predict(self, X):\n","        pass\n","\n","    def loss(self, X_batch, y_batch, reg):\n","        pass\n","\n","class HuberRegression(LinearModel):\n","    \"\"\" Linear regression \"\"\"\n","\n","    def loss(self, X_batch, y_batch, alpha):\n","        return huber_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha=alpha)\n","\n","    def predict(self, X):\n","        # YOUR CODE HERE\n","        y = X.dot(self.w) + self.b\n","        return y"]},{"cell_type":"code","execution_count":11,"id":"766c7c05","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f0baa5b2cf387331bd0a892f09b68ffc","grade":true,"grade_id":"cell-43a5575eb3552466","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"766c7c05","executionInfo":{"status":"ok","timestamp":1702987909093,"user_tz":-60,"elapsed":9750,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7fcae2e8-440f-41c9-ec19-7cbe0f7b7e4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration 0 / 75000: loss 29.345543\n","iteration 10000 / 75000: loss 3.908155\n","iteration 20000 / 75000: loss 3.288578\n","iteration 30000 / 75000: loss 4.352242\n","iteration 40000 / 75000: loss 4.555787\n","iteration 50000 / 75000: loss 4.348727\n","iteration 60000 / 75000: loss 4.211409\n","iteration 70000 / 75000: loss 2.839248\n","MSE scikit-learn: 24.04102301055778\n","MSE gradient descent model : 24.481245240528864\n"]}],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train1 = scaler.fit_transform(X_train1)\n","\n","sk_model = HuberRegressor(fit_intercept=True)\n","sk_model.fit(X_train1, y_train1)\n","sk_pred = sk_model.predict(X_train1)\n","sk_mse = mean_squared_error(sk_pred, y_train1)\n","\n","model = HuberRegression()\n","model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n","pred = model.predict(X_train1)\n","mse = mean_squared_error(pred, y_train1)\n","\n","print(\"MSE scikit-learn:\", sk_mse)\n","print(\"MSE gradient descent model :\", mse)\n","assert mse - sk_mse < 1"]},{"cell_type":"markdown","id":"d4f24bf4","metadata":{"id":"d4f24bf4"},"source":["# Multinomial logistic regression"]},{"cell_type":"code","execution_count":12,"id":"b194affb","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fcf6c9bab4975732f03ce6c8215e72be","grade":false,"grade_id":"cell-27c3352e3785ecdb","locked":true,"schema_version":3,"solution":false,"task":false},"id":"b194affb","executionInfo":{"status":"ok","timestamp":1702987909093,"user_tz":-60,"elapsed":2,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["data = load_iris()\n","X_train2, y_train2 = data.data, data.target\n","\n","W = np.random.randn(X_train2.shape[1], 3) * 0.0001"]},{"cell_type":"code","execution_count":13,"id":"d8405c63","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"af6fecc0ab19bd2c392db4255a946d03","grade":false,"grade_id":"cell-9049b2a8d3edaeaf","locked":false,"schema_version":3,"solution":true,"task":false},"id":"d8405c63","executionInfo":{"status":"ok","timestamp":1702987909093,"user_tz":-60,"elapsed":1,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def softmax_loss_naive(W, X, y, alpha):\n","    \"\"\"\n","    Softmax loss function WITH FOR LOOPS\n","\n","    Inputs:\n","    - W: array of shape (D, C) containing weights\n","    - X: array of shape (N, D) containing a minibatch of data\n","    - y: array of shape (N,) containing training labels\n","    - alpha: (float) regularization\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W;  same shape as W\n","    \"\"\"\n","\n","    # Initialization\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    # Tandremo ny numeric instability\n","    # YOUR CODE HERE\n","    # code from \"https://stackoverflow.com/questions/41663874/cs231n-how-to-calculate-gradient-for-softmax-loss-function\"\n","    # Get shapes\n","    W = W.T\n","    X = X.T\n","    dW = dW.T\n","    num_classes = W.shape[0]\n","    num_train = X.shape[1]\n","\n","    for i in range(num_train):\n","        # Compute vector of scores\n","        f_i = W.dot(X[:, i]) # in R^{num_classes}\n","\n","        # Normalization trick to avoid numerical instability, per http://cs231n.github.io/linear-classify/#softmax\n","        log_c = np.max(f_i)\n","        f_i -= log_c\n","\n","        # Compute loss (and add to it, divided later)\n","        # L_i = - f(x_i)_{y_i} + log \\sum_j e^{f(x_i)_j}\n","        sum_i = 0.0\n","        for f_i_j in f_i:\n","            sum_i += np.exp(f_i_j)\n","        loss += -f_i[y[i]] + np.log(sum_i)\n","\n","        # Compute gradient\n","        # dw_j = 1/num_train * \\sum_i[x_i * (p(y_i = j)-Ind{y_i = j} )]\n","        # Here we are computing the contribution to the inner sum for a given i.\n","        for j in range(num_classes):\n","            p = np.exp(f_i[j])/sum_i\n","            dW[j, :] += (p-(j == y[i])) * X[:, i]\n","\n","    # Compute average\n","    loss /= num_train\n","    dW /= num_train\n","\n","    # Regularization\n","    loss += 0.5 * alpha * np.sum(W * W)\n","    dW += alpha * W\n","\n","    dW = dW.T\n","\n","    return loss, dW"]},{"cell_type":"markdown","id":"8b9aa529","metadata":{"id":"8b9aa529"},"source":["## Without regularization"]},{"cell_type":"code","execution_count":14,"id":"0eea225f","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"ac16981288c3c2f604ad8bab7defad33","grade":true,"grade_id":"cell-878cebbdfa4e3f9b","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"0eea225f","executionInfo":{"status":"ok","timestamp":1702987909812,"user_tz":-60,"elapsed":6,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fdee17c-bff2-472f-80e9-4e1def1b55c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["numerical: 0.278283 analytic: 0.278283, relative error: 5.182412e-10\n","numerical: 0.764740 analytic: 0.764740, relative error: 7.355083e-11\n","numerical: -0.247815 analytic: -0.247815, relative error: 4.548604e-10\n","numerical: 0.095978 analytic: 0.095978, relative error: 4.317675e-11\n","numerical: -0.042116 analytic: -0.042116, relative error: 4.444639e-10\n","numerical: -0.167017 analytic: -0.167017, relative error: 4.011591e-10\n","numerical: 0.278283 analytic: 0.278283, relative error: 5.182412e-10\n","numerical: 0.095978 analytic: 0.095978, relative error: 4.317675e-11\n","numerical: -0.247815 analytic: -0.247815, relative error: 4.548604e-10\n","numerical: 0.317580 analytic: 0.317580, relative error: 2.745532e-11\n","numerical: 0.027985 analytic: 0.027985, relative error: 4.297654e-10\n","numerical: 0.317580 analytic: 0.317580, relative error: 2.745532e-11\n"]}],"source":["loss, dW = softmax_loss_naive(W, X_train2, y_train2, 0.0)\n","\n","f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"]},{"cell_type":"markdown","id":"7edea2f2","metadata":{"id":"7edea2f2"},"source":["## With regularization"]},{"cell_type":"code","execution_count":15,"id":"533001ff","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"bd9a1e462d23666f55369d6b9d152117","grade":true,"grade_id":"cell-ead29b6569263a0f","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"533001ff","executionInfo":{"status":"ok","timestamp":1702987909813,"user_tz":-60,"elapsed":6,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b03b4102-eb2f-47ee-f25f-29e9be0fbe6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["numerical: -0.597858 analytic: -0.597858, relative error: 1.143178e-10\n","numerical: 0.095823 analytic: 0.095823, relative error: 7.128214e-11\n","numerical: -0.166916 analytic: -0.166916, relative error: 4.029421e-10\n","numerical: -0.275121 analytic: -0.275121, relative error: 3.216690e-11\n","numerical: -0.166916 analytic: -0.166916, relative error: 4.029421e-10\n","numerical: -0.030814 analytic: -0.030814, relative error: 4.691109e-09\n","numerical: -0.166916 analytic: -0.166916, relative error: 4.029421e-10\n","numerical: 0.027885 analytic: 0.027885, relative error: 3.826573e-10\n","numerical: -0.124048 analytic: -0.124048, relative error: 2.199070e-10\n","numerical: 0.277835 analytic: 0.277835, relative error: 5.096894e-10\n","numerical: -0.166916 analytic: -0.166916, relative error: 4.029421e-10\n","numerical: 0.095823 analytic: 0.095823, relative error: 7.128214e-11\n"]}],"source":["loss, dW = softmax_loss_naive(W, X_train2, y_train2, 2)\n","\n","f = lambda W: softmax_loss_naive(W, X_train2, y_train2, 2)[0]\n","grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"]},{"cell_type":"code","execution_count":16,"id":"c2593d82","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"48f52a4942e2b468f5775c5c7fbd2617","grade":false,"grade_id":"cell-11e6980597b20334","locked":false,"schema_version":3,"solution":true,"task":false},"id":"c2593d82","executionInfo":{"status":"ok","timestamp":1702987909813,"user_tz":-60,"elapsed":4,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def softmax_loss_vectorized(W, X, y, alpha, fit_intercept=False):\n","    \"\"\"\n","    Softmax loss function WITHOUT FOR LOOPS\n","\n","    Inputs:\n","    - W: array of shape (D, C) containing weights\n","    - X: array of shape (N, D) containing a minibatch of data\n","    - y: array of shape (N,) containing training labels\n","    - alpha: (float) regularization\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W;  same shape as W\n","    \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    # YOUR CODE HERE\n","    # code from \"https://tomaxent.com/2017/03/05/cs231n-Assignment-1-softmax/\"\n","\n","    num_train = X.shape[0]\n","    f = X.dot(W)\n","    f = f - np.max(f, axis=1)[:, np.newaxis]\n","    loss = -np.sum(np.log(np.exp(f[np.arange(num_train), y]) / np.sum(np.exp(f), axis=1)))\n","    loss /= num_train\n","    loss += 0.5 * alpha * np.sum(W * W)\n","    ind = np.zeros_like(f)\n","    ind[np.arange(num_train), y] = 1\n","    dW = X.T.dot(np.exp(f) / np.sum(np.exp(f), axis=1, keepdims=True) - ind)\n","    dW /= num_train\n","    dW += alpha * W\n","\n","    return loss, dW"]},{"cell_type":"markdown","id":"ddea508b","metadata":{"id":"ddea508b"},"source":["## Without regularization"]},{"cell_type":"code","execution_count":17,"id":"78f759e2","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"441d40ba642d3a505582eaeb87981127","grade":true,"grade_id":"cell-0bc0fcf0bd3d3f13","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"78f759e2","executionInfo":{"status":"ok","timestamp":1702987909813,"user_tz":-60,"elapsed":4,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dfec10b9-c98b-4c38-8383-f2e027d9497b"},"outputs":[{"output_type":"stream","name":"stdout","text":["numerical: 0.764740 analytic: 0.764740, relative error: 7.355083e-11\n","numerical: -0.123963 analytic: -0.123963, relative error: 1.382327e-10\n","numerical: 0.278283 analytic: 0.278283, relative error: 4.783455e-10\n","numerical: 0.317580 analytic: 0.317580, relative error: 2.498302e-11\n","numerical: -0.597723 analytic: -0.597723, relative error: 9.099169e-11\n","numerical: 0.027985 analytic: 0.027985, relative error: 6.281263e-10\n","numerical: 0.278283 analytic: 0.278283, relative error: 4.783455e-10\n","numerical: 0.317580 analytic: 0.317580, relative error: 2.498302e-11\n","numerical: -0.167017 analytic: -0.167017, relative error: 3.014491e-10\n","numerical: -0.123963 analytic: -0.123963, relative error: 1.382327e-10\n","numerical: -0.275463 analytic: -0.275463, relative error: 1.899993e-11\n","numerical: 0.027985 analytic: 0.027985, relative error: 6.281263e-10\n"]}],"source":["loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 0.0)\n","\n","f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"]},{"cell_type":"markdown","id":"27c84f53","metadata":{"id":"27c84f53"},"source":["## With regularization"]},{"cell_type":"code","execution_count":18,"id":"8bddf43a","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6475ed3a14441d5488ecb46ec2522bbb","grade":true,"grade_id":"cell-1fe0149d053dade0","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"8bddf43a","executionInfo":{"status":"ok","timestamp":1702987909813,"user_tz":-60,"elapsed":3,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"23fa8d97-e31c-4608-c1e7-708078333461"},"outputs":[{"output_type":"stream","name":"stdout","text":["numerical: -0.042120 analytic: -0.042120, relative error: 2.575045e-11\n","numerical: -0.166916 analytic: -0.166916, relative error: 3.031715e-10\n","numerical: 0.764643 analytic: 0.764643, relative error: 7.763909e-11\n","numerical: -0.030814 analytic: -0.030814, relative error: 4.330810e-09\n","numerical: 0.317497 analytic: 0.317497, relative error: 2.956469e-11\n","numerical: 0.764643 analytic: 0.764643, relative error: 7.763909e-11\n","numerical: -0.166916 analytic: -0.166916, relative error: 3.031715e-10\n","numerical: -0.597858 analytic: -0.597858, relative error: 9.574785e-11\n","numerical: 0.027885 analytic: 0.027885, relative error: 5.817327e-10\n","numerical: 0.317497 analytic: 0.317497, relative error: 2.956469e-11\n","numerical: -0.166916 analytic: -0.166916, relative error: 3.031715e-10\n","numerical: -0.124048 analytic: -0.124048, relative error: 1.304074e-10\n"]}],"source":["loss, dW = softmax_loss_vectorized(W, X_train2, y_train2, 2)\n","\n","f = lambda W: softmax_loss_vectorized(W, X_train2, y_train2, 2)[0]\n","grad_numerical = grad_check_sparse(f, W, dW, error=1e-7)"]},{"cell_type":"markdown","id":"ab7e247a","metadata":{"id":"ab7e247a"},"source":["## Gradient descent"]},{"cell_type":"code","execution_count":19,"id":"9dff0bbc","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a0cedf5c70f5cf04cdd8b74702815dba","grade":false,"grade_id":"cell-0f69bb891603b665","locked":false,"schema_version":3,"solution":true,"task":false},"id":"9dff0bbc","executionInfo":{"status":"ok","timestamp":1702987909813,"user_tz":-60,"elapsed":2,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["class LinearModel():\n","    def __init__(self, fit_intercept=True):\n","        self.W = None\n","        self.fit_intercept = fit_intercept\n","\n","    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n","        if self.fit_intercept:\n","            # YOUR CODE HERE\n","            tmp = np.ones((len(X),1))\n","            X = np.append(tmp, X, axis = 1)\n","\n","        N, d = X.shape\n","\n","        C = (np.max(y) + 1)\n","        if self.W is None: # Initialization\n","            self.W = 0.001 * np.random.randn(d, C)\n","\n","        # Run stochastic gradient descent to optimize W\n","\n","        loss_history = []\n","        for it in range(num_iters):\n","            X_batch = None\n","            y_batch = None\n","\n","            # Sample batch_size elements in X_batch and y_batch\n","            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,)\n","            # Hint: Use np.random.choice to generate indices\n","            # YOUR CODE HERE\n","            rand = np.random.choice(N, batch_size, replace=False)\n","\n","            X_batch = X[rand, :]\n","            y_batch = y[rand]\n","\n","            # evaluate loss and gradient\n","            loss, dW = self.loss(X_batch, y_batch, alpha)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            # Update the weights w using the gradient and the learning rate.\n","            # YOUR CODE HERE\n","            self.W -= learning_rate * dW\n","\n","            if verbose and it % 10000 == 0:\n","                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n","\n","        return loss_history\n","\n","    def predict(self, X):\n","        pass\n","\n","    def loss(self, X_batch, y_batch, reg):\n","        pass\n","\n","class MultinomialLogisticRegressor(LinearModel):\n","    \"\"\" Softmax regression \"\"\"\n","\n","    def loss(self, X_batch, y_batch, alpha):\n","        return softmax_loss_vectorized(self.W, X_batch, y_batch, alpha)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Inputs:\n","        - X: array of shape (N, D)\n","\n","        Returns:\n","        - y_pred: 1-dimensional array of length N, each element is an integer giving the predicted class\n","        \"\"\"\n","        # YOUR CODE HERE\n","        # code from \"towardsdatascience.com/softmax-regression-in-python-multi-class-classificationè3cb560d90cb2?gi=2c4a096fb7e0\"\n","        if self.fit_intercept:\n","            # YOUR CODE HERE\n","            tmp = np.ones((len(X),1))\n","            X = np.append(tmp, X, axis=1)\n","\n","        z = X.dot(self.W)\n","        y_pred = np.argmax(z, axis=1)\n","\n","        return y_pred"]},{"cell_type":"code","execution_count":20,"id":"ce21aaa4","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"56aacc6a9e4ede50cd934a81dfd9915f","grade":true,"grade_id":"cell-8569aecb5759a819","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"ce21aaa4","executionInfo":{"status":"ok","timestamp":1702987919094,"user_tz":-60,"elapsed":9283,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"99441c82-add5-4175-8f63-6e54131f85a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration 0 / 75000: loss 1.098930\n","iteration 10000 / 75000: loss 0.430903\n","iteration 20000 / 75000: loss 0.342827\n","iteration 30000 / 75000: loss 0.319281\n","iteration 40000 / 75000: loss 0.284925\n","iteration 50000 / 75000: loss 0.339942\n","iteration 60000 / 75000: loss 0.317233\n","iteration 70000 / 75000: loss 0.417962\n","Accuracy scikit-learn: 0.86\n","Accuracy gradient descent model : 0.8666666666666667\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train2 = scaler.fit_transform(X_train2)\n","\n","sk_model = LogisticRegression(fit_intercept=False)\n","sk_model.fit(X_train2, y_train2)\n","sk_pred = sk_model.predict(X_train2)\n","sk_accuracy = accuracy_score(y_train2, sk_pred)\n","\n","model = MultinomialLogisticRegressor(fit_intercept=False)\n","model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n","pred = model.predict(X_train2)\n","model_accuracy = accuracy_score(y_train2, pred)\n","\n","print(\"Accuracy scikit-learn:\", sk_accuracy)\n","print(\"Accuracy gradient descent model :\", model_accuracy)\n","assert sk_accuracy - model_accuracy < 0.01"]},{"cell_type":"code","execution_count":21,"id":"b2c6a8ad","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"40a4732103eaf77860c941e0897de01c","grade":true,"grade_id":"cell-30e12569fdfb269c","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"b2c6a8ad","executionInfo":{"status":"ok","timestamp":1702987928806,"user_tz":-60,"elapsed":9714,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"69a39627-f955-4be4-ba46-6475d48b26fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["iteration 0 / 75000: loss 1.098931\n","iteration 10000 / 75000: loss 0.335171\n","iteration 20000 / 75000: loss 0.274202\n","iteration 30000 / 75000: loss 0.223727\n","iteration 40000 / 75000: loss 0.184502\n","iteration 50000 / 75000: loss 0.140211\n","iteration 60000 / 75000: loss 0.190769\n","iteration 70000 / 75000: loss 0.157757\n","Accuracy scikit-learn: 0.9733333333333334\n","Accuracy gradient descent model : 0.96\n"]}],"source":["sk_model = LogisticRegression(fit_intercept=True)\n","sk_model.fit(X_train2, y_train2)\n","sk_pred = sk_model.predict(X_train2)\n","sk_accuracy = accuracy_score(y_train2, sk_pred)\n","\n","model = MultinomialLogisticRegressor(fit_intercept=True)\n","model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n","pred = model.predict(X_train2)\n","model_accuracy = accuracy_score(y_train2, pred)\n","\n","print(\"Accuracy scikit-learn:\", sk_accuracy)\n","print(\"Accuracy gradient descent model :\", model_accuracy)\n","assert sk_accuracy - model_accuracy < 0.02"]},{"cell_type":"markdown","id":"62032646","metadata":{"id":"62032646"},"source":["# K-Nearest Neighbor"]},{"cell_type":"markdown","id":"a3c81034","metadata":{"id":"a3c81034"},"source":["## Computing distances"]},{"cell_type":"code","execution_count":22,"id":"8a97c549","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"858935a2455a3ca0e416b3084b730e7f","grade":false,"grade_id":"cell-9738d380318b956f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8a97c549","executionInfo":{"status":"ok","timestamp":1702987928806,"user_tz":-60,"elapsed":9,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["data = load_digits()\n","X_train3, y_train3 = data.data, data.target\n","X_train3, X_test3, y_train3, y_test3 = train_test_split(X_train3, y_train3, test_size=0.33, random_state=2)\n","\n","def get_distances_two_loops_with_norm(X_train, X_test):\n","    num_test = X_test.shape[0]\n","    num_train = X_train.shape[0]\n","    distances = np.zeros((num_test, num_train))\n","    for i in range(num_test):\n","        for j in range(num_train):\n","            distances[i, j] = np.linalg.norm(X_test[i] - X_train[j])\n","    return distances"]},{"cell_type":"code","execution_count":23,"id":"d1b01336","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"6cfd476a7819d38b3c8ef47443365f64","grade":false,"grade_id":"cell-4756294c792f6985","locked":false,"schema_version":3,"solution":true,"task":false},"id":"d1b01336","executionInfo":{"status":"ok","timestamp":1702987928806,"user_tz":-60,"elapsed":8,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def get_distances_two_loops(X_train, X_test):\n","    \"\"\"\n","    Compute the distance between each test point in X_test and each training point\n","    in X_train\n","\n","    Inputs:\n","    - X_test: array of shape (num_test, D)\n","\n","    Returns:\n","    - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n","    the ith test point and the jth training point.\n","    \"\"\"\n","    num_test = X_test.shape[0]\n","    num_train = X_train.shape[0]\n","    distances = np.zeros((num_test, num_train))\n","    for i in range(num_test):\n","        for j in range(num_train):\n","            # Ataovy ao anaty distances[i, j] ny distance entre ith test point sy th training point\n","            # Aza manao boucle instony ato anatiny\n","            # TSY MAHAZO MAMPIASA np.linalg.norm() :D\n","            # YOUR CODE HERE\n","            distances[i][j] = np.sqrt(np.sum((X_test[i] - X_train[j])**2))\n","\n","    return distances"]},{"cell_type":"code","execution_count":24,"id":"da674674","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"78d07191456013c3a22619c2d26e2503","grade":true,"grade_id":"cell-76b63cae5d6a5977","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"da674674","executionInfo":{"status":"ok","timestamp":1702987936549,"user_tz":-60,"elapsed":7751,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bbc58a7-182a-4a52-8997-3207e78aeafa"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}],"source":["distances = get_distances_two_loops(X_train3, X_test3)\n","true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n","\n","difference = np.linalg.norm(distances - true_distances, ord='fro')\n","\n","print(difference)\n","assert difference < 1e-10"]},{"cell_type":"code","execution_count":25,"id":"69a0a04b","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3b98a31f8ab9e6b8bd5a18ea02dd7165","grade":false,"grade_id":"cell-05658431df004915","locked":false,"schema_version":3,"solution":true,"task":false},"id":"69a0a04b","executionInfo":{"status":"ok","timestamp":1702987936549,"user_tz":-60,"elapsed":9,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def compute_distances_one_loop(X_train, X_test):\n","    \"\"\"\n","    Compute the distance between each test point in X_test and each training point\n","    in X_train\n","\n","    Inputs:\n","    - X_test: array of shape (num_test, D)\n","\n","    Returns:\n","    - dists: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n","    the ith test point and the jth training point.\n","    \"\"\"\n","    num_test = X_test.shape[0]\n","    num_train = X_train.shape[0]\n","    distances = np.zeros((num_test, num_train))\n","    for i in range(num_test):\n","        # Ataovy ao anaty dists[i, j] ny distance entre ith test point sy th training point\n","        # Aza manao boucle instony ato anatiny\n","        # TSY MAHAZO MAMPIASA np.linalg.norm() :D\n","\n","        # YOUR CODE HERE\n","        distances[i] = np.sqrt(np.sum((X_test[i] - X_train)**2, axis=1))\n","    return distances\n","distances = compute_distances_one_loop(X_train3, X_test3)"]},{"cell_type":"code","execution_count":26,"id":"2540b717","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6a9d09964868df2c201e48372082c2e5","grade":true,"grade_id":"cell-cfa64f63da5511f9","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"2540b717","executionInfo":{"status":"ok","timestamp":1702987940276,"user_tz":-60,"elapsed":3735,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95d95d41-8722-43b7-f2ee-e731b5b609f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}],"source":["distances = compute_distances_one_loop(X_train3, X_test3)\n","true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n","\n","difference = np.linalg.norm(distances - true_distances, ord='fro')\n","\n","print(difference)\n","assert difference < 1e-10"]},{"cell_type":"code","execution_count":27,"id":"c3788be0","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"e10c370f1f9c3e653fc817c883b7de0e","grade":false,"grade_id":"cell-3cb10928a6af9889","locked":false,"schema_version":3,"solution":true,"task":false},"id":"c3788be0","executionInfo":{"status":"ok","timestamp":1702987940276,"user_tz":-60,"elapsed":11,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["def get_distances_zero_loop(X_train, X_test):\n","    \"\"\"\n","    Compute the distance between each test point in X_test and each training point\n","    in X_train\n","\n","    Inputs:\n","    - X_test: array of shape (num_test, D)\n","\n","    Returns:\n","    - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n","    the ith test point and the jth training point.\n","    \"\"\"\n","    num_test = X_test.shape[0]\n","    num_train = X_train.shape[0]\n","    distances = np.zeros((num_test, num_train))\n","    # Ataovy ao anaty dists[i, j] ny distance entre ith test point sy th training point\n","    # Aza manao boucle instony\n","    # TSY MAHAZO MAMPIASA np.linalg.norm() NA FONCTIONS AO AMIN'NY SCIPY :D\n","\n","    # YOUR CODE HERE\n","    # code from \"https://programmerall.com/article/1079947341/\"\n","    M = X_test.dot(X_train.T)\n","    nrow = M.shape[0]\n","    ncol = M.shape[1]\n","\n","    te = np.diag(X_test.dot(X_test.T))\n","    tr = np.diag(X_train.dot(X_train.T))\n","\n","    te = np.reshape(np.repeat(te, ncol), M.shape)\n","    tr = np.reshape(np.repeat(tr, nrow), M.T.shape)\n","\n","    sq = -2 * M + te + tr.T\n","    distances = np.sqrt(sq)\n","\n","    return distances\n","\n","distances = get_distances_zero_loop(X_train3, X_test3)"]},{"cell_type":"code","execution_count":28,"id":"81a9294b","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"08a63c2a5bf229c55db5bdc793f5c885","grade":true,"grade_id":"cell-ff999d4dddc0760f","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"81a9294b","executionInfo":{"status":"ok","timestamp":1702987944127,"user_tz":-60,"elapsed":3860,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eadcba16-f0b5-4344-993b-3379959984b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}],"source":["distances = get_distances_zero_loop(X_train3, X_test3)\n","true_distances = get_distances_two_loops_with_norm(X_train3, X_test3)\n","\n","difference = np.linalg.norm(distances - true_distances, ord='fro')\n","\n","print(difference)\n","assert difference < 1e-10"]},{"cell_type":"markdown","id":"bc9c2ac6","metadata":{"id":"bc9c2ac6"},"source":["## K-Nearest Neighbor (knn) classifier"]},{"cell_type":"code","execution_count":29,"id":"2ce1e0f2","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c2213945856056edb1cbc5174c98be4e","grade":false,"grade_id":"cell-8d852d04f1f1e1c6","locked":false,"schema_version":3,"solution":true,"task":false},"id":"2ce1e0f2","executionInfo":{"status":"ok","timestamp":1702987944127,"user_tz":-60,"elapsed":11,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["class KNearestNeighborClassifier():\n","    \"\"\" kNN classifier using L2 distance \"\"\"\n","\n","    def __init__(self, k=1):\n","        \"\"\"\n","        Inputs:\n","        - k: number of nearest neighbors that vote for the predicted labels.\n","        \"\"\"\n","        self.k = k\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Train the classifier. Just memorize the training data.\n","\n","        Inputs:\n","        - X: array of shape (num_train, D)\n","        - y: array of shape (N,)\n","        \"\"\"\n","        self.X_train = X\n","        self.y_train = y\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict labels for test data using this classifier.\n","\n","        Inputs:\n","        - X: array of shape (num_test, D)\n","\n","        Returns:\n","        - y: array of shape (num_test,)\n","        \"\"\"\n","        distances = get_distances_zero_loop(self.X_train, X)\n","        return self.predict_labels(distances)\n","\n","    def predict_labels(self, distances):\n","        \"\"\"\n","        Given a matrix of distances between test points and training points,\n","        predict a label for each test point.\n","\n","        Inputs:\n","        - distances: array of shape (num_test, num_train), dists[i, j] is Euclidean distance between\n","        the ith test point and the jth training point.\n","\n","        Returns:\n","        - y:  array of shape (num_test,)\n","        \"\"\"\n","        num_test = distances.shape[0]\n","        y_pred = np.zeros(num_test)\n","        for i in range(num_test):\n","            # list storing the labels of the k nearest neighbors to the ith test point.\n","            closest_y = []\n","\n","            # Ampidirina ao anaty closest_y ny labels an'ny k neighbors akaiky indrindra\n","            # Jereo fampiasana np.argsort\n","            # YOUR CODE HERE\n","            r = np.argsort(distances[i, :])[:self.k]\n","            closest_y = self.y_train[r]\n","\n","            # Tadiavo ny label betsaka indrindra dia iny no atao prediction\n","            # Raha misy mitovy dia izay label kely raisina\n","            # YOUR CODE HERE\n","            # print(closest_y, \" -> \", np.bincount(closest_y))\n","            y_pred[i] = np.bincount(closest_y).argmax()\n","        return y_pred"]},{"cell_type":"code","execution_count":30,"id":"c419c1ac","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"e5ccea2db11ef9d1310203f6b2227fbc","grade":true,"grade_id":"cell-301db44591c60d17","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"c419c1ac","executionInfo":{"status":"ok","timestamp":1702987944817,"user_tz":-60,"elapsed":700,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9efb0bd3-4aa9-4e1e-a949-721ccc700d32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy scikit-learn: 0.9831649831649831\n","Accuracy gradient descent model : 0.9831649831649831\n"]}],"source":["sk_model = KNeighborsClassifier(n_neighbors=3)\n","sk_model.fit(X_train3, y_train3)\n","sk_pred = sk_model.predict(X_test3)\n","sk_accuracy = accuracy_score(y_test3, sk_pred)\n","\n","model = KNearestNeighborClassifier(k=3)\n","model.fit(X_train3, y_train3)\n","pred = model.predict(X_test3)\n","model_accuracy = accuracy_score(y_test3, pred)\n","\n","print(\"Accuracy scikit-learn:\", sk_accuracy)\n","print(\"Accuracy gradient descent model :\", model_accuracy)\n","assert sk_accuracy - model_accuracy < 1e-10"]},{"cell_type":"markdown","id":"792bbc93","metadata":{"id":"792bbc93"},"source":["## cross-validation"]},{"cell_type":"code","execution_count":31,"id":"d4c44832","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0433a47c347be926b4625f181c01d120","grade":false,"grade_id":"cell-adaaf6c0d8cdce0a","locked":false,"schema_version":3,"solution":true,"task":false},"id":"d4c44832","executionInfo":{"status":"ok","timestamp":1702987949188,"user_tz":-60,"elapsed":4372,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"outputs":[],"source":["num_folds = 5\n","k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n","\n","X_train_folds = []\n","y_train_folds = []\n","\n","# Split up the data into folds\n","# X_train_folds and y_train_folds lits of length num_folds\n","\n","# YOUR CODE HERE\n","X_train, y_train = data.data, data.target\n","\n","X_train_folds = np.array_split(X_train, num_folds)\n","y_train_folds = np.array_split(y_train, num_folds)\n","\n","# A dictionary of length num_folds holding the accuracies for different values of k\n","k_to_accuracies = {}\n","\n","# Ataovy ary ilay k-fold cross validation\n","# Atao ao anaty k_to_accuracies ny accuracy isaky ny valeur k\n","# YOUR CODE HERE\n","\n","# testing data: i_th fold\n","# training data: all other folds\n","for k in k_choices:\n","    model_cv = KNearestNeighborClassifier(k)\n","    k_to_accuracies[k] = np.zeros(num_folds)\n","    for i in range(num_folds):\n","        # test fold X_test_vc | y_train_cv\n","        X_test_cv = X_train_folds[i]\n","        y_test_cv = y_train_folds[i]\n","\n","        # training folds X_train_cv | y_train_cv\n","        X_train_cv = None\n","        y_train_cv = None\n","        for j in range(num_folds):\n","            if i != j:\n","                if X_train_cv is None:\n","                    X_train_cv = X_train_folds[j]\n","                    y_train_cv = y_train_folds[j]\n","                else:\n","                    X_train_cv = np.append(X_train_cv, X_train_folds[j], axis=0)\n","                    y_train_cv = np.append(y_train_cv, y_train_folds[j], axis=0)\n","\n","        # cross validation\n","        model_cv.fit(X_train_cv, y_train_cv)\n","        pred_cv = model_cv.predict(X_test_cv)\n","\n","        # computing accuracy\n","        accuracy = accuracy_score(y_test_cv, pred_cv)\n","        k_to_accuracies[k][i] = accuracy"]},{"cell_type":"code","execution_count":32,"id":"4302f4e6-b007-4b67-9e2c-61c500646ed4","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1f539a48494772e41230a2b590897508","grade":true,"grade_id":"cell-4d0c3c0a08ed3f82","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"id":"4302f4e6-b007-4b67-9e2c-61c500646ed4","executionInfo":{"status":"ok","timestamp":1702987949189,"user_tz":-60,"elapsed":12,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"91551d3a-90dc-40be-9f04-9c4c3b9f60bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["k = 1, accuracy = 0.961111\n","k = 1, accuracy = 0.952778\n","k = 1, accuracy = 0.966574\n","k = 1, accuracy = 0.988858\n","k = 1, accuracy = 0.955432\n","k = 3, accuracy = 0.955556\n","k = 3, accuracy = 0.961111\n","k = 3, accuracy = 0.963788\n","k = 3, accuracy = 0.986072\n","k = 3, accuracy = 0.966574\n","k = 5, accuracy = 0.950000\n","k = 5, accuracy = 0.963889\n","k = 5, accuracy = 0.963788\n","k = 5, accuracy = 0.980501\n","k = 5, accuracy = 0.963788\n","k = 8, accuracy = 0.941667\n","k = 8, accuracy = 0.961111\n","k = 8, accuracy = 0.966574\n","k = 8, accuracy = 0.974930\n","k = 8, accuracy = 0.949861\n","k = 10, accuracy = 0.938889\n","k = 10, accuracy = 0.952778\n","k = 10, accuracy = 0.966574\n","k = 10, accuracy = 0.974930\n","k = 10, accuracy = 0.949861\n","k = 12, accuracy = 0.941667\n","k = 12, accuracy = 0.955556\n","k = 12, accuracy = 0.966574\n","k = 12, accuracy = 0.974930\n","k = 12, accuracy = 0.949861\n","k = 15, accuracy = 0.941667\n","k = 15, accuracy = 0.955556\n","k = 15, accuracy = 0.966574\n","k = 15, accuracy = 0.972145\n","k = 15, accuracy = 0.947075\n","k = 20, accuracy = 0.930556\n","k = 20, accuracy = 0.944444\n","k = 20, accuracy = 0.963788\n","k = 20, accuracy = 0.963788\n","k = 20, accuracy = 0.944290\n","k = 50, accuracy = 0.922222\n","k = 50, accuracy = 0.919444\n","k = 50, accuracy = 0.924791\n","k = 50, accuracy = 0.949861\n","k = 50, accuracy = 0.922006\n","k = 100, accuracy = 0.902778\n","k = 100, accuracy = 0.888889\n","k = 100, accuracy = 0.883008\n","k = 100, accuracy = 0.930362\n","k = 100, accuracy = 0.888579\n"]}],"source":["for k in sorted(k_to_accuracies):\n","    for accuracy in k_to_accuracies[k]:\n","        print('k = %d, accuracy = %f' % (k, accuracy))"]},{"cell_type":"code","source":[],"metadata":{"id":"RrQLCWx970AE","executionInfo":{"status":"ok","timestamp":1702987949189,"user_tz":-60,"elapsed":10,"user":{"displayName":"dino lova hasina Ndimbiarisoa","userId":"10265894513678462314"}}},"id":"RrQLCWx970AE","execution_count":32,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}